{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T18:22:20.671962Z",
     "start_time": "2018-07-20T18:22:12.515714Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rishi\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\rishi\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['machine-learning',\n",
       " 'python',\n",
       " 'neural-network',\n",
       " 'deep-learning',\n",
       " 'classification',\n",
       " 'r',\n",
       " 'data-mining',\n",
       " 'scikit-learn',\n",
       " 'nlp',\n",
       " 'keras',\n",
       " 'clustering',\n",
       " 'dataset',\n",
       " 'tensorflow',\n",
       " 'predictive-modeling',\n",
       " 'time-series',\n",
       " 'regression',\n",
       " 'text-mining',\n",
       " 'statistics',\n",
       " 'bigdata',\n",
       " 'pandas',\n",
       " 'feature-selection',\n",
       " 'visualization',\n",
       " 'convnet',\n",
       " 'random-forest',\n",
       " 'data-cleaning',\n",
       " 'decision-trees',\n",
       " 'algorithms',\n",
       " 'svm',\n",
       " 'data',\n",
       " 'logistic-regression',\n",
       " 'linear-regression',\n",
       " 'rnn',\n",
       " 'recommender-system',\n",
       " 'lstm',\n",
       " 'image-classification',\n",
       " 'xgboost',\n",
       " 'cnn',\n",
       " 'feature-extraction',\n",
       " 'cross-validation',\n",
       " 'reinforcement-learning',\n",
       " 'apache-spark',\n",
       " 'k-means',\n",
       " 'feature-engineering',\n",
       " 'unsupervised-learning',\n",
       " 'computer-vision',\n",
       " 'optimization',\n",
       " 'multiclass-classification',\n",
       " 'gradient-descent',\n",
       " 'word2vec',\n",
       " 'preprocessing',\n",
       " 'prediction',\n",
       " 'supervised-learning',\n",
       " 'categorical-data',\n",
       " 'correlation',\n",
       " 'training',\n",
       " 'word-embeddings',\n",
       " 'backpropagation',\n",
       " 'similarity',\n",
       " 'dimensionality-reduction',\n",
       " 'orange',\n",
       " 'probability',\n",
       " 'pca',\n",
       " 'apache-hadoop',\n",
       " 'beginner',\n",
       " 'loss-function',\n",
       " 'anomaly-detection',\n",
       " 'unbalanced-classes',\n",
       " 'graphs',\n",
       " 'multilabel-classification',\n",
       " 'autoencoder',\n",
       " 'image-recognition',\n",
       " 'convolution',\n",
       " 'matlab',\n",
       " 'accuracy',\n",
       " 'evaluation',\n",
       " 'model-selection',\n",
       " 'naive-bayes-classifier',\n",
       " 'sentiment-analysis',\n",
       " 'performance',\n",
       " 'information-retrieval',\n",
       " 'social-network-analysis',\n",
       " 'text',\n",
       " 'topic-model',\n",
       " 'ensemble-modeling',\n",
       " 'lda',\n",
       " 'normalization',\n",
       " 'feature-scaling',\n",
       " 'outlier',\n",
       " 'gan',\n",
       " 'overfitting',\n",
       " 'nltk',\n",
       " 'machine-learning-model',\n",
       " 'sampling',\n",
       " 'forecast',\n",
       " 'plotting',\n",
       " 'distance',\n",
       " 'pyspark',\n",
       " 'databases',\n",
       " 'hyperparameter',\n",
       " 'tools',\n",
       " 'missing-data',\n",
       " 'regularization',\n",
       " 'q-learning',\n",
       " 'theano',\n",
       " 'numpy',\n",
       " 'terminology',\n",
       " 'reference-request',\n",
       " 'dataframe',\n",
       " 'gpu',\n",
       " 'sql',\n",
       " 'ranking',\n",
       " 'gensim',\n",
       " 'jupyter',\n",
       " 'bayesian',\n",
       " 'named-entity-recognition',\n",
       " 'career',\n",
       " 'gbm',\n",
       " 'class-imbalance',\n",
       " 'confusion-matrix',\n",
       " 'pytorch',\n",
       " 'distribution',\n",
       " 'markov-process',\n",
       " 'natural-language-process',\n",
       " 'data-science-model',\n",
       " 'java',\n",
       " 'sequence',\n",
       " 'online-learning',\n",
       " 'binary',\n",
       " 'cost-function',\n",
       " 'association-rules',\n",
       " 'feature-construction',\n",
       " 'object-recognition',\n",
       " 'recurrent-neural-net',\n",
       " 'generative-models',\n",
       " 'scala',\n",
       " 'audio-recognition',\n",
       " 'encoding',\n",
       " 'stanford-nlp',\n",
       " 'weka',\n",
       " 'search',\n",
       " 'clusters',\n",
       " 'csv',\n",
       " 'tsne',\n",
       " 'sequential-pattern-mining',\n",
       " 'gaussian',\n",
       " 'scraping',\n",
       " 'ab-test',\n",
       " 'parameter-estimation',\n",
       " 'azure-ml',\n",
       " 'education',\n",
       " 'data-imputation',\n",
       " 'k-nn',\n",
       " 'rbm',\n",
       " 'ipython',\n",
       " 'dropout',\n",
       " 'distributed',\n",
       " 'parallel',\n",
       " 'kaggle',\n",
       " 'labels',\n",
       " 'inception',\n",
       " 'software-recommendation',\n",
       " 'bayesian-networks',\n",
       " 'activation-function',\n",
       " 'forecasting',\n",
       " 'mnist',\n",
       " 'descriptive-statistics',\n",
       " 'programming',\n",
       " 'map-reduce',\n",
       " 'efficiency',\n",
       " 'self-study',\n",
       " 'cosine-distance',\n",
       " 'matrix-factorisation',\n",
       " 'geospatial',\n",
       " 'perceptron',\n",
       " 'matplotlib',\n",
       " 'kernel',\n",
       " 'tfidf',\n",
       " 'rstudio',\n",
       " 'scalability',\n",
       " 'parameter',\n",
       " 'definitions',\n",
       " 'language-model',\n",
       " 'research',\n",
       " 'excel',\n",
       " 'object-detection',\n",
       " 'transfer-learning',\n",
       " 'data-analysis',\n",
       " 'caffe',\n",
       " 'nosql',\n",
       " 'aws',\n",
       " 'parsing',\n",
       " 'experiments',\n",
       " 'tableau',\n",
       " 'genetic-algorithms',\n",
       " 'finance',\n",
       " 'information-theory',\n",
       " 'graphical-model',\n",
       " 'dbscan',\n",
       " 'weighted-data',\n",
       " 'similar-documents',\n",
       " 'hyperparameter-tuning',\n",
       " 'semi-supervised-learning',\n",
       " 'data-wrangling',\n",
       " 'machine-translation',\n",
       " 'data-formats',\n",
       " 'knowledge-base',\n",
       " 'open-source',\n",
       " 'apache-mahout',\n",
       " 'marketing',\n",
       " 'ggplot2',\n",
       " 'variance',\n",
       " 'seaborn',\n",
       " 'methodology',\n",
       " 'theory',\n",
       " 'data-augmentation',\n",
       " 'google',\n",
       " 'javascript',\n",
       " 'error-handling',\n",
       " 'scoring',\n",
       " 'books',\n",
       " 'market-basket-analysis',\n",
       " 'ocr',\n",
       " 'linear-algebra',\n",
       " 'classifier',\n",
       " 'mini-batch-gradient-descent',\n",
       " 'metric',\n",
       " 'aggregation',\n",
       " 'simulation',\n",
       " 'hierarchical-data-format',\n",
       " 'data-stream-mining',\n",
       " 'processing',\n",
       " 'multitask-learning',\n",
       " 'churn',\n",
       " 'embeddings',\n",
       " 'tflearn',\n",
       " 'sequence-to-sequence',\n",
       " 'grid-search',\n",
       " 'batch-normalization',\n",
       " 'json',\n",
       " 'fuzzy-logic',\n",
       " 'survival-analysis',\n",
       " 'software-development',\n",
       " 'bioinformatics',\n",
       " 'hive',\n",
       " 'glm',\n",
       " 'libsvm',\n",
       " 'numerical',\n",
       " 'smote',\n",
       " 'math',\n",
       " 'ensemble',\n",
       " 'mlp',\n",
       " 'search-engine',\n",
       " 'neo4j',\n",
       " 'mongodb',\n",
       " 'crawling',\n",
       " 'sas',\n",
       " 'methods',\n",
       " 'library',\n",
       " 'octave',\n",
       " 'text-generation',\n",
       " 'estimators',\n",
       " 'regex',\n",
       " 'amazon-ml',\n",
       " 'torch',\n",
       " 'learning',\n",
       " 'scipy',\n",
       " 'bias',\n",
       " 'speech-to-text',\n",
       " 'yolo',\n",
       " 'homework',\n",
       " 'markov-hidden-model',\n",
       " 'ensemble-learning',\n",
       " 'markov',\n",
       " 'google-prediction-api',\n",
       " 'data.table',\n",
       " 'monte-carlo',\n",
       " 'apache-pig',\n",
       " 'discriminant-analysis',\n",
       " 'metadata',\n",
       " 'domain-adaptation',\n",
       " 'mutual-information',\n",
       " 'score',\n",
       " 'randomized-algorithms',\n",
       " 'expectation-maximization',\n",
       " 'faster-rcnn',\n",
       " 'dqn',\n",
       " 'twitter',\n",
       " 'vector-space-models',\n",
       " 'representation',\n",
       " 'nvidia',\n",
       " 'annotation',\n",
       " 'boosting',\n",
       " 'predictor-importance',\n",
       " 'gridsearchcv',\n",
       " 'automatic-summarization',\n",
       " 'game',\n",
       " 'ngrams',\n",
       " 'anaconda',\n",
       " 'c',\n",
       " 'manifold',\n",
       " 'vc-theory',\n",
       " 'consumerweb',\n",
       " 'indexing',\n",
       " 'notation',\n",
       " 'energy',\n",
       " 'infographics',\n",
       " 'state-of-the-art',\n",
       " 'relational-dbms',\n",
       " 'anonymization',\n",
       " '.net',\n",
       " 'tokenization',\n",
       " 'text-filter',\n",
       " 'management',\n",
       " 'etl',\n",
       " 'powerbi',\n",
       " 'julia',\n",
       " 'convergence',\n",
       " 'interpolation',\n",
       " 'data-product',\n",
       " 'meta-learning',\n",
       " 'cloud-computing',\n",
       " 'lda-classifier',\n",
       " 'learning-rate',\n",
       " 'learning-to-rank',\n",
       " 'vgg16',\n",
       " 'hardware',\n",
       " 'normal-equation',\n",
       " 'generalization',\n",
       " 'svr',\n",
       " 'anomaly',\n",
       " 'actor-critic',\n",
       " 'bayes-error',\n",
       " 'ibm-watson',\n",
       " 'neural-style-transfer',\n",
       " 'noise',\n",
       " 'alex-net',\n",
       " 'historgram',\n",
       " 'tranformation',\n",
       " 'word',\n",
       " 'stata',\n",
       " 'lsi',\n",
       " 'pgm',\n",
       " 'history',\n",
       " 'jaccard-coefficient',\n",
       " 'spss',\n",
       " 'redshift',\n",
       " 'linux',\n",
       " 'data-leakage',\n",
       " 'privacy',\n",
       " 'dirichlet',\n",
       " 'usecase',\n",
       " 'data-indexing-techniques',\n",
       " 'featurization',\n",
       " 'sports',\n",
       " 'version-control',\n",
       " 'genetic',\n",
       " 'hbase',\n",
       " 'freebase',\n",
       " 'community',\n",
       " 'active-learning',\n",
       " 'rattle',\n",
       " 'apache-kafka',\n",
       " 'pybrain',\n",
       " 'cloud',\n",
       " 'pac-learning',\n",
       " 'openai-gym',\n",
       " 'reshape',\n",
       " 'epochs',\n",
       " 'colab',\n",
       " 'vision',\n",
       " 'ai',\n",
       " 'sensors',\n",
       " 'momentum',\n",
       " 'objective-function',\n",
       " 'self-driving',\n",
       " 'cs231n',\n",
       " 'plotly',\n",
       " 'policy-gradients',\n",
       " 'project-planning',\n",
       " 'automation',\n",
       " 'glorot-initialization',\n",
       " 'counts',\n",
       " 'hinge-loss',\n",
       " 'noisification',\n",
       " 'reductions',\n",
       " 'opencpu',\n",
       " 'master-algorithm',\n",
       " 'competitions',\n",
       " 'hog',\n",
       " 'data-munging',\n",
       " 'keras-rl',\n",
       " 'stacked-lstm',\n",
       " 'ndcg',\n",
       " 'fastai',\n",
       " 'pipelines',\n",
       " 'data-drift',\n",
       " 'mesos',\n",
       " 'weight-initialization',\n",
       " 'sensitivity-analysis',\n",
       " 'dplyr',\n",
       " 'stopwords',\n",
       " 'keystroke',\n",
       " 'reci',\n",
       " 'pymc',\n",
       " 'mcmc',\n",
       " 'probabilistic-programming',\n",
       " 'crisp-dm',\n",
       " 'decay',\n",
       " 'design-language',\n",
       " 'information-leakage',\n",
       " 'data-snooping',\n",
       " 'resolution',\n",
       " 'data-visualization',\n",
       " 'hadoop',\n",
       " 'neuralnetwork',\n",
       " 'parallelism',\n",
       " 'pig',\n",
       " 'recommendation',\n",
       " 'scikit',\n",
       " 'sklearn',\n",
       " 'spark']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "USER_AGENT = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64;AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "Tags=[]\n",
    "for num in range(1,13):\n",
    "    response = requests.get('https://datascience.stackexchange.com/tags?page='+repr(num)+'&tab=popular', headers=USER_AGENT,stream=True)\n",
    "    bea_obj=BeautifulSoup(response.content)\n",
    "    Tags.extend([tag.get_text() for tag in bea_obj.find_all('a',{'rel':'tag'})])\n",
    "Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T18:55:00.718109Z",
     "start_time": "2018-07-20T18:54:59.471824Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rishi\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\rishi\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 35]\n",
      "1\n",
      "https://datascience.stackexchange.com/questions/tagged/regression?page=2&sort=newest&pagesize=50\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HTTPResponse' object has no attribute 'peer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-341-ce9fbdda639f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://datascience.stackexchange.com/questions/tagged/regression?page='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpres_page\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'&sort=newest&pagesize=50'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://datascience.stackexchange.com/questions/tagged/regression?page='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpres_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'&sort=newest&pagesize=50'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUSER_AGENT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpeer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mbea_obj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mquestions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbea_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'questions'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HTTPResponse' object has no attribute 'peer'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "proxies = {\n",
    "    'http': 'http://rishicharan96@gmail.com:14101996@192.168.25.1:21218',\n",
    "    'https': 'http://rishicharan96@gmail.com:14101996@192.168.25.1:21218',\n",
    "}\n",
    "USER_AGENT = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64;AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "response = requests.get('https://datascience.stackexchange.com/questions/tagged/python?page=1&sort=newest&pagesize=50', headers=USER_AGENT,stream=True)\n",
    "bea_obj=BeautifulSoup(response.content)\n",
    "pages=bea_obj.find_all('span',{'class':'page-numbers'})\n",
    "page_no=[]\n",
    "if pages:\n",
    "    for page_tag in pages:\n",
    "        try:\n",
    "            page_no.append(int(page_tag.get_text()))\n",
    "        except ValueError:\n",
    "            pass\n",
    "else:\n",
    "    page_no.append(0)\n",
    "ques_list=[]\n",
    "print(page_no)\n",
    "for pres_page in range(1,int((max(page_no)+1)*(15/50))-1):\n",
    "    print(pres_page)\n",
    "    print('https://datascience.stackexchange.com/questions/tagged/regression?page='+repr(pres_page+1)+'&sort=newest&pagesize=50')\n",
    "    response = requests.get('https://datascience.stackexchange.com/questions/tagged/regression?page='+repr(pres_page)+'&sort=newest&pagesize=50', headers=USER_AGENT,stream=True,proxies=proxies)\n",
    "    print(response.raw._original_response.peer)\n",
    "    bea_obj=BeautifulSoup(response.content)\n",
    "    questions=bea_obj.find('div',{'id':'questions'})\n",
    "    if not questions:\n",
    "        break\n",
    "    for question in questions.find_all('div',{'class':'question-summary'}):\n",
    "        temp=[]\n",
    "        try:\n",
    "            temp.append(question['id'].split('-')[-1])     \n",
    "            temp.append(question.find('div',{'class':'views'})['title'].split(' ')[0])\n",
    "            temp.append(question.find_all('strong')[1].get_text())\n",
    "            temp.append(question.find('span',{'class':'reputation-score'}).get_text())\n",
    "            temp.append('https://datascience.stackexchange.com'+question.find('a',{'class':'question-hyperlink'})['href'])\n",
    "            temp.append(question.find('a',{'class':'question-hyperlink'}).get_text())\n",
    "            temp.append(question.find('span',{'class':'relativetime'})['title']) \n",
    "        except:\n",
    "            pass\n",
    "        ques_list.append(temp)\n",
    "ques_df = pd.DataFrame(ques_list,columns=['id','views','answer','reputation-score','url','text','date'])\n",
    "ques_df=ques_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T17:04:13.621394Z",
     "start_time": "2018-07-20T16:59:20.774745Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rishi\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\rishi\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "ans_list=[]\n",
    "for url in list(ques_df[['url','answer']].iterrows()):\n",
    "    if int(url[1]['answer'])>0:\n",
    "        response = requests.get(url[1]['url'],proxies=proxies,headers=USER_AGENT,stream=True)\n",
    "        print(response.raw._original_response)\n",
    "        print(response.status_code)\n",
    "        if int(response.status_code)!= 200:\n",
    "            time.sleep(20)\n",
    "            response = requests.get(url[1]['url'], headers=USER_AGENT,proxies=proxies,tream=True)\n",
    "        bea_obj=BeautifulSoup(response.content)\n",
    "        for answer in bea_obj.find_all('div',{'id':re.compile('^(answer).+')}):\n",
    "            try:\n",
    "                temp=[]\n",
    "                equation=0\n",
    "                st=''\n",
    "                temp.append(answer['data-answerid'])\n",
    "                temp.append(url[0])\n",
    "                temp.append(answer.find(itemprop=\"upvoteCount\").get_text())\n",
    "                temp.append(answer.find('span',{'class':\"relativetime\"})['title'])\n",
    "                for p_tag in answer.find('div',{'class':\"post-text\"}).find_all('p'):\n",
    "                    if p_tag.get_text().find('$')>-1:\n",
    "                        equation=equation+((re.sub('\\$+','\\$',p_tag.get_text()).count('$'))/2)\n",
    "                        st=st+(re.sub('\\$+',' ',p_tag.get_text()))\n",
    "                    else:\n",
    "                        st=st+p_tag.get_text()\n",
    "                temp.append(equation)\n",
    "                temp.append(st)\n",
    "                temp.append(len(answer.find('div',{'class':\"post-text\"}).find_all('code')))\n",
    "                ans_list.append(temp)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T17:04:36.208373Z",
     "start_time": "2018-07-20T17:04:36.200396Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans_df = pd.DataFrame(ans_list,columns=['ans_id','ques_url','vote_count','date','equation','text','code_chunk'])\n",
    "ans_df=ans_df.set_index('ans_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T20:28:17.708294Z",
     "start_time": "2018-07-19T20:28:17.594597Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "knn_ques = ques_df.itertuples()\n",
    "data = tuple(knn_ques)\n",
    "engine = create_engine('sqlite:///C:\\\\Users\\\\rishi\\\\Ml algorithms\\\\ml.db', echo=False)\n",
    "ques_df[400:].to_sql(name='ques_ans', con=engine, if_exists = 'append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
